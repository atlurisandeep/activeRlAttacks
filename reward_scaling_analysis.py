"""
Visualize the reward scaling bug in Active Attacks system.

Run: python reward_scaling_analysis.py
"""
import numpy as np
import matplotlib.pyplot as plt

def current_reward_transform(judge_score, alpha=0.5, length_penalty=2.0, 
                             rep_penalty=3.0, div_penalty=2.0, 
                             repetition_rate=0.3, is_duplicate=False,
                             too_short=False, too_long=False):
    """Current (buggy) implementation."""
    # Step 1: Linear transform
    reward = (judge_score / 10.0) - 0.5  # [-0.5, +0.5]
    
    # Step 2: Reward temperature (compression)
    reward = np.sign(reward) * (np.abs(reward) ** alpha)
    
    # Step 3: Apply penalties (AFTER compression - BUG!)
    penalty = 0.0
    if too_short or too_long:
        penalty -= length_penalty
    if repetition_rate > 0.5:
        penalty -= rep_penalty * repetition_rate
    if is_duplicate:
        penalty -= div_penalty
    
    shaped_reward = reward + penalty
    
    # Step 4: Clip
    shaped_reward = max(-5.0, min(5.0, shaped_reward))
    
    return shaped_reward, reward, penalty


def fixed_reward_transform(judge_score, alpha=0.5, length_penalty=2.0, 
                           rep_penalty=3.0, div_penalty=2.0,
                           repetition_rate=0.3, is_duplicate=False,
                           too_short=False, too_long=False):
    """Fixed implementation."""
    # Step 1: Apply penalties to score (BEFORE compression)
    shaped_score = judge_score
    if too_short or too_long:
        shaped_score -= length_penalty  # in score space (0-10)
    if repetition_rate > 0.5:
        shaped_score -= rep_penalty * repetition_rate
    if is_duplicate:
        shaped_score -= div_penalty
    
    shaped_score = max(0.0, min(10.0, shaped_score))  # clip to [0,10]
    
    # Step 2: Linear transform + compression (ONCE, after penalties)
    reward = (shaped_score / 10.0) - 0.5
    reward = np.sign(reward) * (np.abs(reward) ** alpha)
    
    # Step 3: Clip (wider range)
    reward = max(-10.0, min(10.0, reward))
    
    return reward


# Generate data
judge_scores = np.arange(0, 11, 0.5)

# Scenario 1: Clean attack (no penalties)
current_clean = [current_reward_transform(s)[0] for s in judge_scores]
fixed_clean = [fixed_reward_transform(s) for s in judge_scores]

# Scenario 2: Repetitive attack (rep_rate=0.8)
current_rep = [current_reward_transform(s, repetition_rate=0.8)[0] for s in judge_scores]
fixed_rep = [fixed_reward_transform(s, repetition_rate=0.8) for s in judge_scores]

# Scenario 3: Too short attack
current_short = [current_reward_transform(s, too_short=True)[0] for s in judge_scores]
fixed_short = [fixed_reward_transform(s, too_short=True) for s in judge_scores]

# Scenario 4: All penalties
current_worst = [current_reward_transform(s, too_short=True, repetition_rate=0.8, is_duplicate=True)[0] 
                 for s in judge_scores]
fixed_worst = [fixed_reward_transform(s, too_short=True, repetition_rate=0.8, is_duplicate=True) 
               for s in judge_scores]

# Plot
fig, axes = plt.subplots(2, 2, figsize=(14, 10))
fig.suptitle('Reward Scaling Bug: Current vs Fixed', fontsize=16, fontweight='bold')

# Plot 1: Clean attacks
ax = axes[0, 0]
ax.plot(judge_scores, current_clean, 'r-', linewidth=2, label='Current (buggy)')
ax.plot(judge_scores, fixed_clean, 'g--', linewidth=2, label='Fixed')
ax.axhline(0, color='black', linestyle=':', alpha=0.5)
ax.axvline(5, color='black', linestyle=':', alpha=0.5, label='Success threshold (score=5)')
ax.set_xlabel('Judge Score (0-10)', fontsize=12)
ax.set_ylabel('Final Reward', fontsize=12)
ax.set_title('Scenario 1: Clean Attack (No Penalties)', fontsize=12, fontweight='bold')
ax.legend()
ax.grid(alpha=0.3)

# Plot 2: Repetitive attacks
ax = axes[0, 1]
ax.plot(judge_scores, current_rep, 'r-', linewidth=2, label='Current (buggy)')
ax.plot(judge_scores, fixed_rep, 'g--', linewidth=2, label='Fixed')
ax.axhline(0, color='black', linestyle=':', alpha=0.5)
ax.axvline(5, color='black', linestyle=':', alpha=0.5)
ax.fill_between(judge_scores, -6, 6, where=(np.array(current_rep) < 0) & (np.array(judge_scores) > 5), 
                alpha=0.2, color='red', label='BUG: Successful but penalized')
ax.set_xlabel('Judge Score (0-10)', fontsize=12)
ax.set_ylabel('Final Reward', fontsize=12)
ax.set_title('Scenario 2: Repetitive Attack (rep_rate=0.8)', fontsize=12, fontweight='bold')
ax.legend()
ax.grid(alpha=0.3)

# Plot 3: Too-short attacks
ax = axes[1, 0]
ax.plot(judge_scores, current_short, 'r-', linewidth=2, label='Current (buggy)')
ax.plot(judge_scores, fixed_short, 'g--', linewidth=2, label='Fixed')
ax.axhline(0, color='black', linestyle=':', alpha=0.5)
ax.axvline(5, color='black', linestyle=':', alpha=0.5)
# Highlight dead zone
dead_zone_scores = judge_scores[np.array(current_short) <= -5.0]
if len(dead_zone_scores) > 0:
    ax.axvspan(dead_zone_scores[0], dead_zone_scores[-1], alpha=0.2, color='red', 
               label=f'DEAD ZONE: {len(dead_zone_scores)} scores â†’ -5.0')
ax.set_xlabel('Judge Score (0-10)', fontsize=12)
ax.set_ylabel('Final Reward', fontsize=12)
ax.set_title('Scenario 3: Too-Short Attack (length penalty)', fontsize=12, fontweight='bold')
ax.legend()
ax.grid(alpha=0.3)

# Plot 4: Worst-case (all penalties)
ax = axes[1, 1]
ax.plot(judge_scores, current_worst, 'r-', linewidth=2, label='Current (buggy)')
ax.plot(judge_scores, fixed_worst, 'g--', linewidth=2, label='Fixed')
ax.axhline(0, color='black', linestyle=':', alpha=0.5)
ax.axvline(5, color='black', linestyle=':', alpha=0.5)
# Highlight issue: score=10 still gets negative reward in current
if current_worst[-1] < 0:
    ax.annotate(f'BUG: Score=10 â†’ {current_worst[-1]:.2f}',
                xy=(10, current_worst[-1]), xytext=(8, current_worst[-1] - 1),
                arrowprops=dict(arrowstyle='->', color='red', lw=2),
                fontsize=10, color='red', fontweight='bold')
ax.set_xlabel('Judge Score (0-10)', fontsize=12)
ax.set_ylabel('Final Reward', fontsize=12)
ax.set_title('Scenario 4: All Penalties (short + repetitive + duplicate)', fontsize=12, fontweight='bold')
ax.legend()
ax.grid(alpha=0.3)

plt.tight_layout()
plt.savefig('reward_scaling_bug_visualization.png', dpi=150, bbox_inches='tight')
print("âœ… Saved visualization to: reward_scaling_bug_visualization.png")

# Print numerical analysis
print("\n" + "="*80)
print("NUMERICAL ANALYSIS: Current vs Fixed")
print("="*80)

print("\nScenario 2 (Repetitive Attack):")
print("-" * 60)
for score in [0, 3, 5, 7, 10]:
    curr = current_reward_transform(score, repetition_rate=0.8)[0]
    fix = fixed_reward_transform(score, repetition_rate=0.8)
    print(f"Judge Score={score:2d}: Current={curr:+6.2f}  Fixed={fix:+6.2f}  Diff={fix-curr:+6.2f}")

print("\nScenario 4 (All Penalties):")
print("-" * 60)
for score in [0, 3, 5, 7, 10]:
    curr = current_reward_transform(score, too_short=True, repetition_rate=0.8, is_duplicate=True)[0]
    fix = fixed_reward_transform(score, too_short=True, repetition_rate=0.8, is_duplicate=True)
    print(f"Judge Score={score:2d}: Current={curr:+6.2f}  Fixed={fix:+6.2f}  Diff={fix-curr:+6.2f}")

print("\nðŸš¨ CRITICAL ISSUES HIGHLIGHTED:")
print("-" * 60)
print("1. Dead Zone: Scores 0-3 with penalties all clip to -5.0 (no gradient signal)")
print("2. Perverse Incentive: Score=10 with penalties â†’ negative reward")
print("3. Penalty Dominance: Penalties (-7.0 total) >> base reward (Â±0.7)")
print("4. Asymmetric Range: Positive rewards maxed at +0.7, negative at -5.0 (7x asymmetry)")
print("\nâœ… Fixed version addresses all issues by applying penalties in score space.")
print("="*80)
